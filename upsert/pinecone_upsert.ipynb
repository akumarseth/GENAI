{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('asia-southeast1-gcp-free',\n",
       " 'd5914e95-5cec-429a-82dc-7df31911a212',\n",
       " 'genaiembedding01',\n",
       " 'text-embedding-ada-002',\n",
       " 'genaichatgpt',\n",
       " 'gpt-3.5-turbo',\n",
       " '2022-12-01')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "EMBEDDING_DEPLOYMENT_NAME=os.getenv(\"EMBEDDING_DEPLOYMENT_NAME\")\n",
    "EMBEDDING_MODEL_NAME=os.getenv(\"EMBEDDING_MODEL_NAME\")\n",
    "CHAT_DEPLOYMENT_NAME=os.getenv(\"CHAT_DEPLOYMENT_NAME\")\n",
    "CHAT_MODEL_NAME=os.getenv(\"CHAT_MODEL_NAME\")\n",
    "OPENAI_API_VERSION=os.getenv(\"OPENAI_API_VERSION\")\n",
    "\n",
    "PINECONE_ENV,PINECONE_API_KEY,EMBEDDING_DEPLOYMENT_NAME,EMBEDDING_MODEL_NAME,CHAT_DEPLOYMENT_NAME,CHAT_MODEL_NAME,OPENAI_API_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.document_loaders import TextLoader, Docx2txtLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 694, which is longer than the specified 500\n",
      "Created a chunk of size 569, which is longer than the specified 500\n",
      "Created a chunk of size 568, which is longer than the specified 500\n",
      "Created a chunk of size 1821, which is longer than the specified 500\n",
      "Created a chunk of size 1892, which is longer than the specified 500\n",
      "Created a chunk of size 650, which is longer than the specified 500\n",
      "Created a chunk of size 503, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "loader = Docx2txtLoader(\"../docs/langchain_docs.docx\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(deployment=EMBEDDING_DEPLOYMENT_NAME,chunk_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', deployment='genaiembedding01', openai_api_version='2022-12-01', openai_api_base='https://atom-openai-dev01.openai.azure.com/', openai_api_type='azure', openai_proxy='', embedding_ctx_length=8191, openai_api_key='722b5c0e90e34cc082f09aa917188e80', openai_organization='', allowed_special=set(), disallowed_special='all', chunk_size=1, max_retries=6, request_timeout=None, headers=None, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_ENV,  # next to api key in console\n",
    ")\n",
    "\n",
    "index_name = \"demo1\"\n",
    "\n",
    "docsearch = Pinecone.from_documents(docs, embeddings, index_name=index_name)\n",
    "\n",
    "# if you already have an index, you can load it like this\n",
    "# docsearch = Pinecone.from_existing_index(index_name, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Introduction\\n\\nLangChain\\xa0is a framework for developing applications powered by language models. It enables applications that are:\\n\\nData-aware: connect a language model to other sources of data\\n\\nAgentic: allow a language model to interact with its environment\\n\\nThe main value props of LangChain are:', metadata={'source': '../docs/langchain_docs.docx'}),\n",
       "  0.88699013),\n",
       " (Document(page_content='Introduction\\n\\nLangChain\\xa0is a framework for developing applications powered by language models. It enables applications that are:\\n\\nData-aware: connect a language model to other sources of data\\n\\nAgentic: allow a language model to interact with its environment\\n\\nThe main value props of LangChain are:\\n\\nComponents: abstractions for working with language models, along with a collection of implementations for each abstraction. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not\\n\\nOff-the-shelf chains: a structured assembly of components for accomplishing specific higher-level tasks\\n\\nOff-the-shelf chains make it easy to get started. For more complex applications and nuanced use-cases, components make it easy to customize existing chains or build new ones.\\n\\nGet started\\u200b\\n\\nHere’s\\xa0how to install LangChain, set up your environment, and start building.', metadata={'source': '../docs/langchain_intro.docx'}),\n",
       "  0.865284681),\n",
       " (Document(page_content='Off-the-shelf chains make it easy to get started. For more complex applications and nuanced use-cases, components make it easy to customize existing chains or build new ones.\\n\\nGet started\\u200b\\n\\nHere’s\\xa0how to install LangChain, set up your environment, and start building.\\n\\nWe recommend following our\\xa0Quickstart\\xa0guide to familiarize yourself with the framework by building your first LangChain application.', metadata={'source': '../docs/langchain_docs.docx'}),\n",
       "  0.853330851),\n",
       " (Document(page_content='We recommend following our\\xa0Quickstart\\xa0guide to familiarize yourself with the framework by building your first LangChain application.\\n\\nNote: These docs are for the LangChain\\xa0Python package. For documentation on\\xa0LangChain.js, the JS/TS version,\\xa0head here.\\n\\nModules\\u200b\\n\\nLangChain provides standard, extendable interfaces and external integrations for the following modules, listed from least to most complex:\\n\\nModel I/O\\u200b\\n\\nInterface with language models\\n\\nData connection\\u200b\\n\\nInterface with application-specific data\\n\\nChains\\u200b\\n\\nConstruct sequences of calls\\n\\nAgents\\u200b\\n\\nLet chains choose which tools to use given high-level directives\\n\\nMemory\\u200b\\n\\nPersist application state between runs of a chain\\n\\nCallbacks\\u200b\\n\\nLog and stream intermediate steps of any chain\\n\\nExamples, ecosystem, and resources\\u200b\\n\\nUse cases\\u200b\\n\\nWalkthroughs and best-practices for common end-to-end use cases, like:\\n\\nChatbots\\n\\nAnswering questions using sources\\n\\nAnalyzing structured data\\n\\nand much more...\\n\\nGuides\\u200b', metadata={'source': '../docs/langchain_intro.docx'}),\n",
       "  0.848127246)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Langchain?\"\n",
    "docs = docsearch.similarity_search_with_score(query)\n",
    "docs\n",
    "# print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='The new way of programming models is through prompts. A\\xa0prompt\\xa0refers to the input to the model. This input is often constructed from multiple components. LangChain provides several classes and functions to make constructing and working with prompts easy.\\n\\nPrompt templates: Parametrize model inputs\\n\\nExample selectors: Dynamically select examples to include in prompts\\n\\nPrompt templates', metadata={'source': '../docs/langchain_docs.docx'}),\n",
       "  0.814887226),\n",
       " (Document(page_content='Language models take text as input - that text is commonly referred to as a prompt. Typically this is not simply a hardcoded string but rather a combination of a template, some examples, and user input. LangChain provides several classes and functions to make constructing and working with prompts easy.\\n\\nWhat is a prompt template?\\u200b', metadata={'source': '../docs/langchain_docs.docx'}),\n",
       "  0.806539237),\n",
       " (Document(page_content='A prompt template refers to a reproducible way to generate a prompt. It contains a text string (\"the template\"), that can take in a set of parameters from the end user and generates a prompt.\\n\\nA prompt template can contain:\\n\\ninstructions to the language model,\\n\\na set of few shot examples to help the language model generate a better response,\\n\\na question to the language model.\\n\\nHere\\'s the simplest example:\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tfrom langchain import PromptTemplate', metadata={'source': '../docs/langchain_docs.docx'}),\n",
       "  0.804511666),\n",
       " (Document(page_content='There are essentially two distinct prompt templates available - string prompt templates and chat prompt templates. String prompt templates provides a simple prompt in string format, while chat prompt templates produces a more structured prompt to be used with a chat API.\\n\\nIn this guide, we will create a custom prompt using a string prompt template.\\n\\nTo create a custom string prompt template, there are two requirements:', metadata={'source': '../docs/langchain_docs.docx'}),\n",
       "  0.795134187)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is prompt engineering and what is best practice?\"\n",
    "docs = docsearch.similarity_search_with_score(query)\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_poc_genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
